# Implementation of DNN from scratch using numpy

This notebook is inspired by Coursera Deep Learning Course 1 assignment on writing Dense Neural Network from scratch using numpy.

I wrote this as an exercise to see if I could implement multiple layers of DNNs by writing forward propagation and backward propagation functions and by using a cache for each layer to store parameter, input and output data necessary for backpropagation calculations.

Just like the notebook in Andrew NG's assignment, I tested the L-layer Neural Network by using a cat/non-cat dataset to see how well it performs binary classification.

I also use the same L-layer DNN code to classify datapoints with nonlinear classification boundaries and it works pretty well!
